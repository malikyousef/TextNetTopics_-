{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI0WbeCsaGLn"
      },
      "source": [
        "# **Topic Models for Regular-Sized Documents**\n",
        "**Daniel Voskergian, Rashid Jayouse and Malik Yousef**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**To perform topic modeling** --> A csv file containing pre-processed dataset is required. Pre-processing means removing punctuations, numbers, stop-words, stemming, words less than n char, etc. Each line contains one document.\n",
        "\n",
        "*Note:* for embedding-based topic models, we use a non-stemmed dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PQfaG9hg803R"
      },
      "outputs": [],
      "source": [
        "file_name = '/content/title+abstract_1.csv' #stemmed dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dX2txjFmlPL"
      },
      "outputs": [],
      "source": [
        "file_name_ns = '/content/title+abstract_2.csv' #not_stemmed dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1ab4pBrqAQs"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# ***A) Algebraic Topic Models***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fFLydWHsNUY"
      },
      "source": [
        "# **NMF - Non-negative Matrix Factorization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qx7hWULntbXC"
      },
      "outputs": [],
      "source": [
        "!pip uninstall scikit-learn -y\n",
        "!pip install scikit-learn==0.24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tAL33JrLsl4S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMZ2n9aaspRz"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(file_name, encoding = \"ISO-8859-1\", header = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "O0RCc1xWsufC"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "analyzer = TfidfVectorizer().build_analyzer()\n",
        "\n",
        "# Override TfidfVectorizer\n",
        "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
        "    def build_analyzer(self):\n",
        "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
        "        return lambda doc: analyzer(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pr3zURr5s5sJ"
      },
      "outputs": [],
      "source": [
        "vectorizer = StemmedTfidfVectorizer(min_df=50)\n",
        "matrix = vectorizer.fit_transform(data[0])\n",
        "words_df = pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names())\n",
        "words_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMV8VhpytRSi"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import NMF\n",
        "\n",
        "model = NMF(n_components=20)\n",
        "model.fit(matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSSG1KFcvPWm"
      },
      "outputs": [],
      "source": [
        "n_words = 20\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "\n",
        "topic_list = []\n",
        "for topic_idx, topic in enumerate(model.components_):\n",
        "   top_features = [feature_names[i] for i in topic.argsort()][::-1][:n_words]  \n",
        "   top_n = ' '.join(top_features)\n",
        "   topic_list.append(f\"topic_{'_'.join(top_features[:3])}\") \n",
        "\n",
        "   print(f\"Topic {topic_idx}: {top_n}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "yWCuUxv9vcZ8"
      },
      "outputs": [],
      "source": [
        "n_words = 20\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "\n",
        "topic_list = []\n",
        "with open('NMF_topics_words.txt', 'w') as f:\n",
        "  for topic_idx, topic in enumerate(model.components_):\n",
        "     top_features = [feature_names[i] for i in topic.argsort()][::-1][:n_words]\n",
        "     for i in top_features:\n",
        "      f.write(i)\n",
        "      f.write('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_HPpZet3vk3y"
      },
      "outputs": [],
      "source": [
        "amounts = model.transform(matrix) \n",
        "probs_df=pd.DataFrame(amounts)\n",
        "probs_df.to_excel(r'NMF_topics_distibutions.xlsx', sheet_name='NMF', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX52RjG5y2g4"
      },
      "source": [
        "#**LSI - Latent Semantic Analysis/Indexing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Y7lF8Yp0zJxY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(file_name, header = None) "
      ],
      "metadata": {
        "id": "0OsCJCaYzhhM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "L7XSaLPwz4r_"
      },
      "outputs": [],
      "source": [
        "# vectorizers for creating the document-term-matrix (DTM)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Cd8nTP7lz7WB"
      },
      "outputs": [],
      "source": [
        "vect =TfidfVectorizer() # to play with. min_df,max_df,max_features etc..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "MVlMdPeZz9XQ"
      },
      "outputs": [],
      "source": [
        "vect_text=vect.fit_transform(data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "K_QEr3Xq0KMx"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "lsa_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=1000, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "QnMszNhP0Myo"
      },
      "outputs": [],
      "source": [
        "lsa_top=lsa_model.fit_transform(vect_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wTrbZJp0QPC"
      },
      "outputs": [],
      "source": [
        "tmp=\"\"\n",
        "# most important words for each topic\n",
        "vocab = vect.get_feature_names()\n",
        "import codecs\n",
        "# top words of each topic\n",
        "file = codecs.open('LSI_topics_words','w','utf-8')\n",
        "\n",
        "with open('LSIreadme.txt', 'w') as f:\n",
        " for i, comp in enumerate(lsa_model.components_):\n",
        "    \n",
        "    vocab_comp = zip(vocab, comp)\n",
        "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:20]\n",
        "    print(\"Topic \"+str(i)+\": \")\n",
        "    for t in sorted_words:\n",
        "        f.write(t[0])\n",
        "        f.write('\\n')\n",
        "        print(t[0],end=\" \")\n",
        "        tmp += t[0] + ' '\n",
        "    file.write(tmp + '\\n')\n",
        "    tmp = ''\n",
        "    \n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtM2XUhKvq4B"
      },
      "source": [
        "# **FLSA-W - Fuzzy Latent Semantic Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C81i4hmPvsqm"
      },
      "outputs": [],
      "source": [
        "pip install FuzzyTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqiT8vA7vud2"
      },
      "outputs": [],
      "source": [
        "from FuzzyTM import FLSA_W"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H1CyyvvwdeH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(file_name) "
      ],
      "metadata": {
        "id": "DULTI9tez3RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtnsGkbCwjDP"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3P1qy05wkvC"
      },
      "outputs": [],
      "source": [
        "data['tokenized_sents'] = data.apply(lambda row: nltk.word_tokenize(row[0]), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgRTR97Jws_b"
      },
      "outputs": [],
      "source": [
        "data['tokenized_sents'].values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LF1Jdh5awxj0"
      },
      "outputs": [],
      "source": [
        "flsaW = FLSA_W(input_file = data['tokenized_sents'].values.tolist(), num_topics=20, num_words=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0bwurFEw0zX"
      },
      "outputs": [],
      "source": [
        "pwgt, ptgd = flsaW.get_matrices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7Vm87xOw2ln"
      },
      "outputs": [],
      "source": [
        "topics = flsaW.show_topics(representation='words', num_words=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAHAv-8Gw4AZ"
      },
      "outputs": [],
      "source": [
        "with open('FLSA_topics_words.txt', 'w') as f:\n",
        "  for i in range(20):\n",
        "    print(\"\\n\")\n",
        "    for m in range(20):\n",
        "      print(topics[i][m])\n",
        "      f.write(topics[i][m])\n",
        "      f.write('\\n')\n",
        "print(topics[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOGtQyqSw6nQ"
      },
      "outputs": [],
      "source": [
        "probs_df=pd.DataFrame(ptgd)\n",
        "probs_df.to_excel(r'FLSA_topics_distibutions.xlsx', sheet_name='FLSA', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1EcTJVNq9hd"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# ***B) Probabilistic Topic Models***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkcagElxtsUX"
      },
      "source": [
        "# **PLSA - Probabilistic Latent Semantic Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15F9wo3ltume"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqS1_Kw0tvbO"
      },
      "outputs": [],
      "source": [
        "%matplotlib notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGGApT15tzKm"
      },
      "outputs": [],
      "source": [
        "sys.path.append('..')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5yxEtHVt1J2"
      },
      "outputs": [],
      "source": [
        "!pip install plsa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K39cWquEt352"
      },
      "outputs": [],
      "source": [
        "from plsa import Corpus, Pipeline, Visualize\n",
        "from plsa.pipeline import DEFAULT_PIPELINE\n",
        "from plsa.algorithms import PLSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jFm--ldt5f-"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cFGS2Mjt9kG"
      },
      "outputs": [],
      "source": [
        "!pip install preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiHBZdvHt-Ue"
      },
      "outputs": [],
      "source": [
        "#There is a need to change some Pipline class parameters \n",
        "pipeline = Pipeline(* DEFAULT_PIPELINE)\n",
        "pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjb7K3jCuBim"
      },
      "outputs": [],
      "source": [
        "corpus = Corpus.from_csv(file_name, pipeline)\n",
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TZo9zrHuDRe"
      },
      "outputs": [],
      "source": [
        "n_topics = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIIEha7WuFhX"
      },
      "outputs": [],
      "source": [
        "plsa = PLSA(corpus, n_topics, True)\n",
        "plsa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoAGTvjduIQ_"
      },
      "outputs": [],
      "source": [
        "result = plsa.fit()\n",
        "plsa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ks-vJFuiyP5v"
      },
      "outputs": [],
      "source": [
        "probs= result.topic_given_doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7Y7bFoVuM2H"
      },
      "outputs": [],
      "source": [
        "with open('PLSA_topics_words.txt', 'w') as f:\n",
        "  for i in range(20):\n",
        "    topic_words = result.word_given_topic[i][:20] \n",
        "    print(\"\\n\")\n",
        "    for m in range(20):\n",
        "      print(topic_words[m][0])\n",
        "      f.write(topic_words[m][0])\n",
        "      f.write('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK--H2SmuUon"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "probs_df=pd.DataFrame(probs)\n",
        "probs_df.to_excel(r'PLSA_topics_distibutions.xlsx', sheet_name='PLSA', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyby_s_P3Wp6"
      },
      "source": [
        "# **CTM - Correlated Topic Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDleeVmm7clb"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install tomotopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqLLLYd17mxu"
      },
      "outputs": [],
      "source": [
        "import tomotopy as tp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQqf48i17p9C"
      },
      "outputs": [],
      "source": [
        "# k = number of topics\n",
        "mdl = tp.CTMModel(k=20)\n",
        "for line in open(file_name , encoding = \"ISO-8859-1\"):\n",
        "    mdl.add_doc(line.strip().split())\n",
        "\n",
        "for i in range(0, 100, 10):\n",
        "    mdl.train(10)\n",
        "    print('Iteration: {}\\tLog-likelihood: {}'.format(i, mdl.ll_per_word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CExNOfD7bdns"
      },
      "outputs": [],
      "source": [
        "#Showing topic-word distribution lists\n",
        "for k in range(mdl.k):\n",
        "    print('Top 20 words of topic #{}'.format(k))\n",
        "    print(mdl.get_topic_words(k, top_n=20))  #top_m = number of words per topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOcRfcGoaaLE"
      },
      "outputs": [],
      "source": [
        "#Showing only words in each topic\n",
        "for k in range(mdl.k):\n",
        "    print()\n",
        "    for i in range(20):\n",
        "      print(mdl.get_topic_words(k, top_n=20)[i][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxL-jdQ0-BQM"
      },
      "outputs": [],
      "source": [
        "#inference \n",
        "topic_distribution=[]\n",
        "for line in open(file_name_ns, encoding = \"ISO-8859-1\"):\n",
        "  doc_inst = mdl.make_doc(line.strip().split())\n",
        "  b, t = mdl.infer(doc_inst)\n",
        "  topic_distribution.append(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7vsYCrqBevd"
      },
      "outputs": [],
      "source": [
        "# Saving topic distribution over documents in excel sheet\n",
        "import pandas as pd\n",
        "probs_df=pd.DataFrame(topic_distribution)\n",
        "probs_df.to_excel(r'ts_CorrelatedTM_distibutions_T2.xlsx', sheet_name='CorrelatedTM', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7wjOxXPrFsC"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# ***C) Embedding-based Topic Models***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJWDraPu7LHp"
      },
      "source": [
        "# **BERTopics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R115Jux7TGW"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade joblib==1.1.0; \n",
        "#Restart Runtime after executing this cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFjrHlbM8aP8"
      },
      "outputs": [],
      "source": [
        "!pip install bertopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVjkUc2V8u1z"
      },
      "outputs": [],
      "source": [
        "from bertopic import BERTopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CWIwumN8yla"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "data = pd.read_csv(file_name_ns, header = None) #write header name if exists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cShqy4ua9bDq"
      },
      "outputs": [],
      "source": [
        "topic_model = BERTopic(calculate_probabilities=True, top_n_words=20, min_topic_size=50)         # nr_topics=20, top_n_words=20,\n",
        "topic_model_large = BERTopic(\"all-mpnet-base-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQbbbiMl9kfK"
      },
      "outputs": [],
      "source": [
        "topics, probs = topic_model.fit_transform(data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CWs5uzSAGoo"
      },
      "outputs": [],
      "source": [
        "all_topics = topic_model.get_topics()\n",
        "print(len(all_topics))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_6ynEMp-o2h"
      },
      "outputs": [],
      "source": [
        "frequency = topic_model.get_topic_freq()\n",
        "frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oZlwn0S-3XB"
      },
      "outputs": [],
      "source": [
        "# Further reduce topics (if needed)\n",
        "topic_model.reduce_topics(data[0], nr_topics=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYptrbks_MuE"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_barchart(n_words=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS-_znc1_c94"
      },
      "outputs": [],
      "source": [
        "with open('BERTopic_topic_words.txt', 'w') as f:\n",
        "  for i in range(len(all_topics)-1):\n",
        "    topic_words = topic_model.get_topic(i)\n",
        "    print(\"\\n\")\n",
        "    for m in range(20):\n",
        "      print(topic_words[m][0])\n",
        "      f.write(topic_words[m][0])\n",
        "      f.write('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jol4Dw-PBD4x"
      },
      "outputs": [],
      "source": [
        "probs_df=pd.DataFrame(probs)\n",
        "probs_df.to_excel(r'BERTopics_topics_distributions.xlsx', sheet_name='BERTopics', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klfKwHzvrWIb"
      },
      "source": [
        "# **Top2Vec**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FK4l2RAqrcY5"
      },
      "outputs": [],
      "source": [
        "!pip install top2vec --no-cache-dir --no-binary :all:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQcAqVRyrfkB"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import json\n",
        "import os\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import clear_output, display\n",
        "from top2vec import Top2Vec\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R0zdCmTr1Ef"
      },
      "outputs": [],
      "source": [
        "metadata_df = pd.read_csv(file_name_ns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MzecUSxr_7n"
      },
      "outputs": [],
      "source": [
        "top2vec = Top2Vec(documents=metadata_df[0].values.tolist(), speed=\"learn\", workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nS1Cq04osO12"
      },
      "outputs": [],
      "source": [
        "top2vec.get_num_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GO_hT66GsHK9"
      },
      "outputs": [],
      "source": [
        "topic_mapping = top2vec.hierarchical_topic_reduction(num_topics=20)\n",
        "topic_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcJGV9TlsK9O"
      },
      "outputs": [],
      "source": [
        "topic_distribution=[]\n",
        "for i in range(len(metadata_df)):\n",
        "  topic_distribution.insert(i,top2vec.get_documents_topics([i], reduced=True, num_topics=20)[1].tolist()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqCeUoYPsgym"
      },
      "outputs": [],
      "source": [
        "with open('T2v1_topics_words.txt', 'w') as f:\n",
        "  for i in range(45):\n",
        "    topic_words = top2vec.get_topics(45, reduced=False)[0][i]\n",
        "    print(\"\\n\")\n",
        "    for m in range(20):\n",
        "      print(topic_words[m])\n",
        "      f.write(topic_words[m])\n",
        "      f.write('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MHqEt1psnU2"
      },
      "outputs": [],
      "source": [
        "probs_df=pd.DataFrame(topic_distribution)\n",
        "probs_df.to_excel(r'T2V_topics_distibutions.xlsx', sheet_name='T2V', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n96OA86rmK02"
      },
      "source": [
        "# **CombinedTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBvjgXfNmMqW"
      },
      "outputs": [],
      "source": [
        "pip install -U contextualized_topic_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_58iBRVmPaB"
      },
      "outputs": [],
      "source": [
        "from contextualized_topic_models.models.ctm import CombinedTM\n",
        "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
        "from contextualized_topic_models.utils.data_preparation import bert_embeddings_from_file\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIBNu3w1mXEd"
      },
      "outputs": [],
      "source": [
        "qt = TopicModelDataPreparation(\"all-mpnet-base-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-l9cFRkmauV"
      },
      "outputs": [],
      "source": [
        "list_of_preprocessed_documents = pd.read_csv(file_name, header = None)[0].values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0zlwO-inStJ"
      },
      "outputs": [],
      "source": [
        "list_of_unpreprocessed_documents = pd.read_csv(file_name_ns, header = None)[0].values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNcyUKo9n_te"
      },
      "outputs": [],
      "source": [
        "training_dataset = qt.fit(text_for_contextual=list_of_unpreprocessed_documents, text_for_bow=list_of_preprocessed_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsGrESVyosmZ"
      },
      "outputs": [],
      "source": [
        "ctm = CombinedTM(bow_size=len(qt.vocab), contextual_size=768, n_components=20) # 50 topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ytlt1QkDo4EU"
      },
      "outputs": [],
      "source": [
        "ctm.fit(training_dataset) # run the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mY7azLR2o6rN"
      },
      "outputs": [],
      "source": [
        "with open('CombinedTM_topics_words.txt', 'w') as f:\n",
        " for k in range(20):\n",
        "    for i in range(20):\n",
        "      print(ctm.get_topics(20)[k][i])\n",
        "      f.write(ctm.get_topics(20)[k][i])\n",
        "      f.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbxsF3YlpLJs"
      },
      "source": [
        "# **EmbdedTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2Zr7MtepFRO"
      },
      "outputs": [],
      "source": [
        "pip install -U embedded_topic_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xojP_PuIpUHe"
      },
      "outputs": [],
      "source": [
        "from embedded_topic_model.utils import preprocessing\n",
        "import json\n",
        "\n",
        "# Loading a dataset in JSON format. As said, documents must be composed by string sentences\n",
        "corpus_file = '/content/abstract_full_filtered_no_stopwords.json'\n",
        "documents_raw = json.load(open(corpus_file, 'r'))\n",
        "documents = [document['List(Term as String)'] for document in documents_raw]\n",
        "\n",
        "# Preprocessing the dataset\n",
        "vocabulary, train_dataset, _, = preprocessing.create_etm_datasets(\n",
        "    documents, \n",
        "    min_df=0.01, \n",
        "    max_df=0.75, \n",
        "    train_size=0.85, \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBpeLD55phYG"
      },
      "outputs": [],
      "source": [
        "from embedded_topic_model.utils import embedding\n",
        "\n",
        "# Training word2vec embeddings\n",
        "embeddings_mapping = embedding.create_word2vec_embedding_from_dataset(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKA2PvzSpkgt"
      },
      "outputs": [],
      "source": [
        "from embedded_topic_model.models.etm import ETM\n",
        "\n",
        "# Training an ETM instance\n",
        "etm_instance = ETM(\n",
        "    vocabulary,\n",
        "    embeddings=embeddings_mapping, # You can pass here the path to a word2vec file or\n",
        "                                   # a KeyedVectors instance\n",
        "    num_topics=20,\n",
        "    num_words=20,\n",
        "    epochs=300,\n",
        "    debug_mode=True,\n",
        "    train_embeddings=False, # Optional. If True, ETM will learn word embeddings jointly with\n",
        "                            # topic embeddings. By default, is False. If 'embeddings' argument\n",
        "                            # is being passed, this argument must not be True\n",
        ")\n",
        "\n",
        "etm_instance.fit(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsfzlzPmprL9"
      },
      "outputs": [],
      "source": [
        "topics = etm_instance.get_topics(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GU9YAFbxptu_"
      },
      "outputs": [],
      "source": [
        "with open('ETM_topics_words.txt', 'w') as f:\n",
        " for k in range(20):\n",
        "    for i in range(20):\n",
        "      print(topics[k][i])\n",
        "      f.write(topics[k][i])\n",
        "      f.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LDA2VEC**"
      ],
      "metadata": {
        "id": "2J2koNO_1O_q"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}